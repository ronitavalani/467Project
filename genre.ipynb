{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronitavalani/467Project/blob/main/genre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 2: Load and preprocess dataset\n",
        "url = 'https://raw.githubusercontent.com/ronitavalani/467Project/main/songs_normalize.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Keep first genre only\n",
        "df['genre'] = df['genre'].astype(str).apply(lambda x: x.split(',')[0].strip())\n",
        "\n",
        "# Drop non-numeric columns (except genre)\n",
        "non_numeric_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "non_numeric_cols.remove('genre')\n",
        "df = df.drop(columns=non_numeric_cols)\n",
        "\n",
        "# Drop NA rows\n",
        "df = df.dropna()\n",
        "\n",
        "# Feature matrix and labels\n",
        "X = df.drop(columns=['genre'])\n",
        "y = df['genre']\n",
        "\n",
        "# Drop genre classes with <2 examples\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "value_counts = pd.Series(y_encoded).value_counts()\n",
        "valid_classes = value_counts[value_counts > 1].index\n",
        "valid_mask = pd.Series(y_encoded).isin(valid_classes)\n",
        "X = X[valid_mask]\n",
        "y = y[valid_mask].reset_index(drop=True)\n",
        "\n",
        "# Re-encode after filtering\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Step 3: Custom Dataset\n",
        "class SongDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.X = torch.tensor(features, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = SongDataset(X_train, y_train)\n",
        "test_dataset = SongDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Step 4: Neural Network with One Hidden Layer\n",
        "class GenreNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GenreNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Model, loss, optimizer\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 64\n",
        "output_dim = len(np.unique(y_encoded))\n",
        "\n",
        "model = GenreNet(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Step 5: Training Loop\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Step 6: Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "print(f\"\\nTest Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "8MufPDFc1LCJ",
        "outputId": "0212fffe-6585-4155-97bd-a36f35bf6a1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1.9561\n",
            "Epoch 2/20, Loss: 1.3303\n",
            "Epoch 3/20, Loss: 1.1162\n",
            "Epoch 4/20, Loss: 1.0425\n",
            "Epoch 5/20, Loss: 1.0102\n",
            "Epoch 6/20, Loss: 0.9920\n",
            "Epoch 7/20, Loss: 0.9769\n",
            "Epoch 8/20, Loss: 0.9634\n",
            "Epoch 9/20, Loss: 0.9540\n",
            "Epoch 10/20, Loss: 0.9441\n",
            "Epoch 11/20, Loss: 0.9352\n",
            "Epoch 12/20, Loss: 0.9259\n",
            "Epoch 13/20, Loss: 0.9178\n",
            "Epoch 14/20, Loss: 0.9110\n",
            "Epoch 15/20, Loss: 0.9029\n",
            "Epoch 16/20, Loss: 0.8951\n",
            "Epoch 17/20, Loss: 0.8878\n",
            "Epoch 18/20, Loss: 0.8818\n",
            "Epoch 19/20, Loss: 0.8752\n",
            "Epoch 20/20, Loss: 0.8686\n",
            "\n",
            "Test Accuracy: 66.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries and load data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Load the CSV file\n",
        "url = 'https://raw.githubusercontent.com/ronitavalani/467Project/main/songs_normalize.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Preprocess the genre column\n",
        "df['genre'] = df['genre'].astype(str).apply(lambda x: x.split(',')[0].strip())\n",
        "\n",
        "# Drop rows with missing values just in case\n",
        "df = df.dropna()\n",
        "\n",
        "# Step 3: Check genre distribution\n",
        "print(\"Genre distribution:\\n\", df['genre'].value_counts())\n",
        "\n",
        "# Drop non-numeric columns except 'genre'\n",
        "non_numeric_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "non_numeric_cols.remove('genre')  # keep genre as label\n",
        "X = df.drop(columns=non_numeric_cols + ['genre'])  # drop all other string columns\n",
        "y = df['genre']\n",
        "\n",
        "# Encode genre labels\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Drop classes with fewer than 2 samples (needed for stratify split)\n",
        "genre_counts = pd.Series(y_encoded).value_counts()\n",
        "valid_classes = genre_counts[genre_counts > 1].index\n",
        "valid_mask = pd.Series(y_encoded).isin(valid_classes)\n",
        "\n",
        "X_scaled = X_scaled[valid_mask]\n",
        "y_filtered = y[valid_mask].reset_index(drop=True)  # Use original genre names, then re-encode\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y_filtered)\n",
        "\n",
        "# Recalculate class weights after filtering\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_encoded),\n",
        "    y=y_encoded\n",
        ")\n",
        "class_weights = dict(enumerate(weights))\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Build basic neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(len(np.unique(y_encoded)), activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# Evaluate on test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "3tObtm6SFNdx",
        "outputId": "1d20f95d-1060-4969-97a7-74ee4616e95a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genre distribution:\n",
            " genre\n",
            "pop                  936\n",
            "hip hop              776\n",
            "rock                 162\n",
            "Dance/Electronic      41\n",
            "set()                 22\n",
            "latin                 15\n",
            "R&B                   13\n",
            "country               11\n",
            "World/Traditional     10\n",
            "metal                  9\n",
            "Folk/Acoustic          4\n",
            "easy listening         1\n",
            "Name: count, dtype: int64\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0893 - loss: 2.3990 - val_accuracy: 0.1000 - val_loss: 2.3686\n",
            "Epoch 2/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1012 - loss: 2.5405 - val_accuracy: 0.0531 - val_loss: 2.3690\n",
            "Epoch 3/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1007 - loss: 2.4176 - val_accuracy: 0.0469 - val_loss: 2.3548\n",
            "Epoch 4/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0942 - loss: 2.3547 - val_accuracy: 0.0719 - val_loss: 2.3449\n",
            "Epoch 5/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1038 - loss: 2.1877 - val_accuracy: 0.1031 - val_loss: 2.3008\n",
            "Epoch 6/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1247 - loss: 2.4140 - val_accuracy: 0.1250 - val_loss: 2.2966\n",
            "Epoch 7/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1544 - loss: 2.3603 - val_accuracy: 0.1969 - val_loss: 2.2496\n",
            "Epoch 8/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2069 - loss: 2.2533 - val_accuracy: 0.2125 - val_loss: 2.2093\n",
            "Epoch 9/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2611 - loss: 1.6599 - val_accuracy: 0.2500 - val_loss: 2.1318\n",
            "Epoch 10/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2558 - loss: 1.6467 - val_accuracy: 0.2375 - val_loss: 2.1271\n",
            "Epoch 11/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2640 - loss: 1.8164 - val_accuracy: 0.2750 - val_loss: 2.0688\n",
            "Epoch 12/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2770 - loss: 1.6019 - val_accuracy: 0.2875 - val_loss: 2.0392\n",
            "Epoch 13/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2924 - loss: 1.4431 - val_accuracy: 0.3281 - val_loss: 1.9786\n",
            "Epoch 14/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3274 - loss: 1.6423 - val_accuracy: 0.2781 - val_loss: 2.0145\n",
            "Epoch 15/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3132 - loss: 1.2124 - val_accuracy: 0.2969 - val_loss: 1.9272\n",
            "Epoch 16/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3014 - loss: 1.5510 - val_accuracy: 0.2719 - val_loss: 1.9590\n",
            "Epoch 17/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3281 - loss: 1.3087 - val_accuracy: 0.2781 - val_loss: 1.9671\n",
            "Epoch 18/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2945 - loss: 1.4644 - val_accuracy: 0.2656 - val_loss: 1.9315\n",
            "Epoch 19/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2730 - loss: 1.1860 - val_accuracy: 0.2969 - val_loss: 1.9060\n",
            "Epoch 20/20\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3137 - loss: 1.4094 - val_accuracy: 0.2969 - val_loss: 1.8746\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3075 - loss: 1.8443  \n",
            "\n",
            "Test Accuracy: 0.30\n"
          ]
        }
      ]
    }
  ]
}